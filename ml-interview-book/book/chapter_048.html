<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Hessian Matrix in Optimization: Why Deep Learning Avoids Second-Order Methods - Machine Learning Interview Questions: Complete Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Comprehensive answers to 189 ML interview questions from top tech companies">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Machine Learning Interview Questions: Complete Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/wingston/MLJobSearch2025" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/wingston/MLJobSearch2025/edit/main/ml-interview-book/src/chapter_048.md" title="Suggest an edit" aria-label="Suggest an edit" rel="edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="the-hessian-matrix-in-optimization-why-deep-learning-avoids-second-order-methods"><a class="header" href="#the-hessian-matrix-in-optimization-why-deep-learning-avoids-second-order-methods">The Hessian Matrix in Optimization: Why Deep Learning Avoids Second-Order Methods</a></h1>
<h2 id="the-interview-question"><a class="header" href="#the-interview-question">The Interview Question</a></h2>
<blockquote>
<p><strong>Tech Company</strong>: "What is the role of the Hessian matrix in optimization, and why is it not commonly used in training deep neural networks?"</p>
</blockquote>
<h2 id="why-this-question-matters"><a class="header" href="#why-this-question-matters">Why This Question Matters</a></h2>
<p>This question appears in machine learning interviews at top tech companies because it tests several critical competencies:</p>
<ul>
<li><strong>Deep mathematical understanding</strong>: Beyond basic gradient descent, can you explain second-order optimization methods?</li>
<li><strong>Computational complexity awareness</strong>: Do you understand the practical constraints of large-scale machine learning?</li>
<li><strong>Trade-off analysis</strong>: Can you evaluate when theoretical advantages don't translate to practical benefits?</li>
<li><strong>Modern optimization knowledge</strong>: Are you familiar with why the field evolved away from certain approaches?</li>
</ul>
<p>Companies like Google, Microsoft, and Amazon ask this question because it reveals whether candidates can navigate the gap between elegant mathematical theory and messy computational reality - a crucial skill for production machine learning systems.</p>
<h2 id="fundamental-concepts"><a class="header" href="#fundamental-concepts">Fundamental Concepts</a></h2>
<h3 id="what-is-the-hessian-matrix"><a class="header" href="#what-is-the-hessian-matrix">What is the Hessian Matrix?</a></h3>
<p>Think of the Hessian matrix as a mathematical tool that captures the "curvature" of a function in multiple dimensions. Just as a single second derivative tells you whether a curve bends upward or downward at a point, the Hessian matrix tells you how a multi-dimensional function curves in all possible directions.</p>
<p><strong>Key terminology:</strong></p>
<ul>
<li><strong>First derivative (gradient)</strong>: Shows the steepest direction uphill</li>
<li><strong>Second derivative</strong>: Shows how quickly the slope is changing</li>
<li><strong>Hessian matrix</strong>: A square matrix of all possible second derivatives for a multi-variable function</li>
</ul>
<h3 id="mathematical-structure"><a class="header" href="#mathematical-structure">Mathematical Structure</a></h3>
<p>For a function <code>f(x₁, x₂, ..., xₙ)</code> with n variables, the Hessian matrix H is defined as:</p>
<pre><code>H[i,j] = ∂²f / (∂xᵢ ∂xⱼ)
</code></pre>
<p>This means each entry in the matrix represents how the function curves when you change two specific variables simultaneously.</p>
<p><strong>Important properties:</strong></p>
<ul>
<li>Always square (n×n for n parameters)</li>
<li>Symmetric (H[i,j] = H[j,i]) when second derivatives are continuous</li>
<li>Positive definite at local minima (curves upward in all directions)</li>
<li>Negative definite at local maxima (curves downward in all directions)</li>
</ul>
<h2 id="detailed-explanation"><a class="header" href="#detailed-explanation">Detailed Explanation</a></h2>
<h3 id="the-role-of-the-hessian-in-optimization"><a class="header" href="#the-role-of-the-hessian-in-optimization">The Role of the Hessian in Optimization</a></h3>
<p>Imagine you're hiking in foggy mountains and trying to find the lowest valley. With only a gradient (first derivative), you know which direction leads downhill, but you don't know if you're on a gentle slope or about to hit a cliff.</p>
<p>The Hessian matrix provides this crucial "curvature information":</p>
<ol>
<li><strong>Direction of steepest descent</strong>: The gradient points downhill</li>
<li><strong>Rate of change</strong>: The Hessian tells you how quickly the landscape changes</li>
<li><strong>Local shape</strong>: Is this a narrow valley or a broad basin?</li>
</ol>
<h3 id="newtons-method-the-theoretical-ideal"><a class="header" href="#newtons-method-the-theoretical-ideal">Newton's Method: The Theoretical Ideal</a></h3>
<p>Newton's method uses both gradient and Hessian information:</p>
<pre><code>x_{new} = x_{old} - H⁻¹ × ∇f
</code></pre>
<p>Where:</p>
<ul>
<li><code>∇f</code> is the gradient (first derivatives)</li>
<li><code>H⁻¹</code> is the inverse of the Hessian matrix</li>
</ul>
<p><strong>Why this works so well theoretically:</strong></p>
<ul>
<li>Takes larger steps in directions where the function curves gently</li>
<li>Takes smaller steps where the function curves sharply</li>
<li>Achieves "quadratic convergence" - error decreases quadratically with each step</li>
<li>Often reaches the optimum in just a few iterations</li>
</ul>
<h3 id="a-simple-example"><a class="header" href="#a-simple-example">A Simple Example</a></h3>
<p>Consider optimizing the function <code>f(x,y) = x² + 10y²</code>:</p>
<p><strong>Gradient:</strong> <code>∇f = [2x, 20y]</code>
<strong>Hessian:</strong> <code>H = [[2, 0], [0, 20]]</code></p>
<p>The Hessian tells us the function curves 10 times more steeply in the y-direction than x-direction. Newton's method automatically adjusts step sizes accordingly, while gradient descent treats both directions equally and zigzags inefficiently.</p>
<h2 id="mathematical-foundations"><a class="header" href="#mathematical-foundations">Mathematical Foundations</a></h2>
<h3 id="understanding-second-derivatives-intuitively"><a class="header" href="#understanding-second-derivatives-intuitively">Understanding Second Derivatives Intuitively</a></h3>
<p>Think of driving a car:</p>
<ul>
<li><strong>Position</strong>: The function value</li>
<li><strong>Velocity</strong>: First derivative (gradient) - how fast you're moving</li>
<li><strong>Acceleration</strong>: Second derivative - how quickly your speed changes</li>
</ul>
<p>The Hessian captures "acceleration" in all possible directions simultaneously.</p>
<h3 id="computational-requirements"><a class="header" href="#computational-requirements">Computational Requirements</a></h3>
<p>For a neural network with n parameters:</p>
<ul>
<li><strong>Gradient storage</strong>: O(n) memory</li>
<li><strong>Hessian storage</strong>: O(n²) memory</li>
<li><strong>Hessian computation</strong>: O(n²) or O(n³) operations</li>
<li><strong>Matrix inversion</strong>: O(n³) operations</li>
</ul>
<h3 id="real-numbers-example"><a class="header" href="#real-numbers-example">Real Numbers Example</a></h3>
<p>Consider a modest deep network with 1 million parameters:</p>
<ul>
<li><strong>Gradient</strong>: 1 million numbers (4 MB in single precision)</li>
<li><strong>Hessian</strong>: 1 trillion numbers (4 TB in single precision!)</li>
</ul>
<p>For GPT-3 with 175 billion parameters, the Hessian would require over 100 exabytes of storage - more than all data stored by humanity combined.</p>
<h2 id="practical-applications"><a class="header" href="#practical-applications">Practical Applications</a></h2>
<h3 id="where-hessian-methods-excel"><a class="header" href="#where-hessian-methods-excel">Where Hessian Methods Excel</a></h3>
<p><strong>Small-scale optimization problems:</strong></p>
<ul>
<li>Classical statistics (logistic regression with hundreds of features)</li>
<li>Engineering design optimization</li>
<li>Scientific computing with well-behaved functions</li>
</ul>
<p><strong>Specific algorithms using Hessian information:</strong></p>
<ul>
<li>Newton-Raphson method</li>
<li>Levenberg-Marquardt algorithm</li>
<li>Trust region methods</li>
</ul>
<h3 id="why-deep-learning-uses-alternatives"><a class="header" href="#why-deep-learning-uses-alternatives">Why Deep Learning Uses Alternatives</a></h3>
<p><strong>Stochastic Gradient Descent (SGD) and variants:</strong></p>
<pre><code class="language-python"># Simple gradient descent update
parameters = parameters - learning_rate * gradient

# Adam optimizer (popular variant)
# Uses exponential moving averages of gradients
# Adapts learning rates per parameter
</code></pre>
<p><strong>Key advantages:</strong></p>
<ul>
<li>Memory efficient: O(n) instead of O(n²)</li>
<li>Computationally fast: O(n) per iteration</li>
<li>Works with mini-batches and stochastic optimization</li>
<li>Naturally handles non-convex landscapes</li>
</ul>
<h3 id="modern-compromises-quasi-newton-methods"><a class="header" href="#modern-compromises-quasi-newton-methods">Modern Compromises: Quasi-Newton Methods</a></h3>
<p><strong>Limited-memory BFGS (L-BFGS):</strong></p>
<ul>
<li>Approximates Hessian using past gradients</li>
<li>Stores only last few gradient differences</li>
<li>Memory requirement: O(n) instead of O(n²)</li>
</ul>
<p><strong>Hessian-free optimization:</strong></p>
<ul>
<li>Computes Hessian-vector products without storing full Hessian</li>
<li>Uses conjugate gradient method for Newton step</li>
<li>Practical for medium-sized networks</li>
</ul>
<h2 id="common-misconceptions-and-pitfalls"><a class="header" href="#common-misconceptions-and-pitfalls">Common Misconceptions and Pitfalls</a></h2>
<h3 id="misconception-1-second-order-methods-are-always-better"><a class="header" href="#misconception-1-second-order-methods-are-always-better">Misconception 1: "Second-order methods are always better"</a></h3>
<p><strong>Reality</strong>: Theoretical convergence rates don't account for computational cost per iteration. In deep learning, taking 1000 cheap gradient steps often beats 10 expensive Newton steps.</p>
<h3 id="misconception-2-the-hessian-is-too-hard-to-compute"><a class="header" href="#misconception-2-the-hessian-is-too-hard-to-compute">Misconception 2: "The Hessian is too hard to compute"</a></h3>
<p><strong>Reality</strong>: We can compute Hessian-vector products efficiently using automatic differentiation, but the storage requirement remains prohibitive for large networks.</p>
<h3 id="misconception-3-modern-optimizers-dont-use-second-order-information"><a class="header" href="#misconception-3-modern-optimizers-dont-use-second-order-information">Misconception 3: "Modern optimizers don't use second-order information"</a></h3>
<p><strong>Reality</strong>: Optimizers like Adam and RMSprop use diagonal approximations of the Hessian through adaptive learning rates per parameter.</p>
<h3 id="misconception-4-first-order-methods-are-always-slower"><a class="header" href="#misconception-4-first-order-methods-are-always-slower">Misconception 4: "First-order methods are always slower"</a></h3>
<p><strong>Reality</strong>: In stochastic settings with noisy gradients, the precise second-order information becomes less valuable, and robust first-order methods often perform better.</p>
<h2 id="interview-strategy"><a class="header" href="#interview-strategy">Interview Strategy</a></h2>
<h3 id="structure-your-answer"><a class="header" href="#structure-your-answer">Structure Your Answer</a></h3>
<ol>
<li>
<p><strong>Define the Hessian clearly</strong>: "The Hessian matrix contains all second-order partial derivatives of a function, capturing how the function curves in all directions."</p>
</li>
<li>
<p><strong>Explain its optimization role</strong>: "In Newton's method, the Hessian provides curvature information that allows for more informed optimization steps, potentially achieving faster convergence."</p>
</li>
<li>
<p><strong>Address the computational reality</strong>: "For deep networks with millions of parameters, storing and computing the full Hessian becomes computationally prohibitive due to O(n²) memory requirements."</p>
</li>
<li>
<p><strong>Mention practical alternatives</strong>: "Modern deep learning uses first-order methods like SGD and Adam, or approximation techniques like L-BFGS for smaller networks."</p>
</li>
</ol>
<h3 id="key-points-to-emphasize"><a class="header" href="#key-points-to-emphasize">Key Points to Emphasize</a></h3>
<ul>
<li>Computational complexity scaling (O(n²) vs O(n))</li>
<li>Memory requirements for large networks</li>
<li>Trade-off between theoretical convergence and practical efficiency</li>
<li>Stochastic optimization challenges</li>
</ul>
<h3 id="follow-up-questions-to-expect"><a class="header" href="#follow-up-questions-to-expect">Follow-up Questions to Expect</a></h3>
<p><strong>"How would you approximate second-order information efficiently?"</strong></p>
<ul>
<li>Mention diagonal Hessian approximations</li>
<li>Discuss quasi-Newton methods</li>
<li>Explain Hessian-vector products</li>
</ul>
<p><strong>"Are there any modern uses of second-order methods in deep learning?"</strong></p>
<ul>
<li>Natural gradients in reinforcement learning</li>
<li>Shampoo optimizer research</li>
<li>Specific applications in smaller networks</li>
</ul>
<p><strong>"What about adaptive learning rate methods like Adam?"</strong></p>
<ul>
<li>Explain how they implicitly use second-order information</li>
<li>Discuss the connection to diagonal Hessian approximations</li>
</ul>
<h3 id="red-flags-to-avoid"><a class="header" href="#red-flags-to-avoid">Red Flags to Avoid</a></h3>
<ul>
<li>Claiming Hessian methods are "impossible" to use (they're just impractical for large networks)</li>
<li>Ignoring the stochastic nature of deep learning optimization</li>
<li>Focusing only on theory without practical considerations</li>
<li>Not mentioning memory and computational constraints</li>
</ul>
<h2 id="related-concepts"><a class="header" href="#related-concepts">Related Concepts</a></h2>
<h3 id="optimization-landscape"><a class="header" href="#optimization-landscape">Optimization Landscape</a></h3>
<p>Understanding the Hessian connects to broader concepts:</p>
<ul>
<li><strong>Saddle points</strong>: Where gradient is zero but Hessian has both positive and negative eigenvalues</li>
<li><strong>Conditioning</strong>: Ill-conditioned Hessians lead to optimization difficulties</li>
<li><strong>Loss surface geometry</strong>: How neural network loss functions behave in high dimensions</li>
</ul>
<h3 id="automatic-differentiation"><a class="header" href="#automatic-differentiation">Automatic Differentiation</a></h3>
<p>Modern deep learning frameworks use:</p>
<ul>
<li><strong>Forward-mode AD</strong>: Efficient for computing directional derivatives</li>
<li><strong>Reverse-mode AD</strong>: Efficient for gradients (backpropagation)</li>
<li><strong>Higher-order AD</strong>: Can compute Hessian-vector products</li>
</ul>
<h3 id="information-geometry"><a class="header" href="#information-geometry">Information Geometry</a></h3>
<p>Advanced topics connecting Hessian concepts:</p>
<ul>
<li><strong>Natural gradients</strong>: Using the Fisher information matrix instead of Hessian</li>
<li><strong>Riemannian optimization</strong>: Optimization on curved manifolds</li>
<li><strong>K-FAC</strong>: Kronecker-factored approximations to natural gradients</li>
</ul>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<h3 id="academic-papers"><a class="header" href="#academic-papers">Academic Papers</a></h3>
<ul>
<li>"Deep Learning via Hessian-free Optimization" by Martens (2010) - Seminal work on making second-order methods practical for neural networks</li>
<li>"On the importance of initialization and momentum in deep learning" by Sutskever et al. (2013) - Insights into when second-order information helps</li>
</ul>
<h3 id="books-and-tutorials"><a class="header" href="#books-and-tutorials">Books and Tutorials</a></h3>
<ul>
<li>"Numerical Optimization" by Nocedal &amp; Wright - Comprehensive coverage of optimization theory</li>
<li>"Deep Learning" by Goodfellow, Bengio &amp; Courville - Chapter 8 covers optimization for machine learning</li>
<li>"Convex Optimization" by Boyd &amp; Vandenberghe - Mathematical foundations</li>
</ul>
<h3 id="online-resources"><a class="header" href="#online-resources">Online Resources</a></h3>
<ul>
<li>Andrew Gibiansky's blog post on Hessian-free optimization</li>
<li>CS231n Stanford lectures on optimization</li>
<li>Distill.pub articles on optimization in deep learning</li>
</ul>
<h3 id="practical-implementations"><a class="header" href="#practical-implementations">Practical Implementations</a></h3>
<ul>
<li>PyTorch's automatic differentiation documentation</li>
<li>JAX tutorials on higher-order derivatives</li>
<li>TensorFlow's second-order optimization examples</li>
</ul>
<hr />
<p><em>This chapter provides the foundational knowledge needed to confidently discuss Hessian matrices in machine learning interviews, bridging theoretical understanding with practical implementation constraints that drive real-world deep learning systems.</em></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="chapter_047.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="chapter_061.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="chapter_047.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="chapter_061.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
