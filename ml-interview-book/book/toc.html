<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- sidebar iframe generated using mdBook

        This is a frame, and not included directly in the page, to control the total size of the
        book. The TOC contains an entry for each page, so if each page includes a copy of the TOC,
        the total size of the page becomes O(n**2).

        The frame is only used as a fallback when JS is turned off. When it's on, the sidebar is
        instead added to the main page by `toc.js` instead. The JavaScript mode is better
        because, when running in a `file:///` URL, the iframed page would not be Same-Origin as
        the rest of the page, so the sidebar and the main page theme would fall out of sync.
        -->
        <meta charset="UTF-8">
        <meta name="robots" content="noindex">
        <!-- Custom HTML head -->
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom.css">
    </head>
    <body class="sidebar-iframe-inner">
        <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html" target="_parent">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Fundamentals</li><li class="chapter-item expanded "><a href="chapter_001.html" target="_parent"><strong aria-hidden="true">1.</strong> Why We Use Smaller Learning Rates: The Key to Stable ML Training</a></li><li class="chapter-item expanded "><a href="chapter_002.html" target="_parent"><strong aria-hidden="true">2.</strong> Train-Test Split Ratios: Beyond the 80:20 Rule</a></li><li class="chapter-item expanded "><a href="chapter_003.html" target="_parent"><strong aria-hidden="true">3.</strong> Understanding Covariance vs Correlation: A Complete Guide for ML Interviews</a></li><li class="chapter-item expanded "><a href="chapter_004.html" target="_parent"><strong aria-hidden="true">4.</strong> Understanding Mean, Median, and Mode in Skewed Distributions</a></li><li class="chapter-item expanded "><a href="chapter_005.html" target="_parent"><strong aria-hidden="true">5.</strong> Loss Function Robustness: Understanding MAE vs MSE vs RMSE with Outliers</a></li><li class="chapter-item expanded "><a href="chapter_036.html" target="_parent"><strong aria-hidden="true">6.</strong> Understanding Type I and Type II Errors: The Foundation of Statistical Decision Making</a></li><li class="chapter-item expanded "><a href="chapter_063.html" target="_parent"><strong aria-hidden="true">7.</strong> Understanding Dependence vs. Correlation: A Statistical Foundation for Machine Learning</a></li><li class="chapter-item expanded "><a href="chapter_068.html" target="_parent"><strong aria-hidden="true">8.</strong> The Law of Large Numbers: Foundation of Statistical Reliability</a></li><li class="chapter-item expanded "><a href="chapter_069.html" target="_parent"><strong aria-hidden="true">9.</strong> Understanding Selection Bias: The Hidden Threat to Machine Learning Models</a></li><li class="chapter-item expanded affix "><li class="part-title">Data Preprocessing and Feature Engineering</li><li class="chapter-item expanded "><a href="chapter_035.html" target="_parent"><strong aria-hidden="true">10.</strong> Handling Missing Values in High-Missing-Rate Datasets</a></li><li class="chapter-item expanded "><a href="chapter_049.html" target="_parent"><strong aria-hidden="true">11.</strong> Combining Mixed-Dimensional Features for Classification and Regression</a></li><li class="chapter-item expanded "><a href="chapter_057.html" target="_parent"><strong aria-hidden="true">12.</strong> What Happens to Variance When Data is Duplicated?</a></li><li class="chapter-item expanded "><a href="chapter_075.html" target="_parent"><strong aria-hidden="true">13.</strong> Mutual Information Filtering: Understanding Redundant Feature Selection</a></li><li class="chapter-item expanded "><a href="chapter_089.html" target="_parent"><strong aria-hidden="true">14.</strong> Handling Missing Data: A Complete Guide to Imputation Strategies</a></li><li class="chapter-item expanded "><a href="chapter_092.html" target="_parent"><strong aria-hidden="true">15.</strong> Feature Engineering: The Art of Transforming Raw Data into ML Gold</a></li><li class="chapter-item expanded affix "><li class="part-title">Neural Networks and Deep Learning</li><li class="chapter-item expanded "><a href="chapter_013.html" target="_parent"><strong aria-hidden="true">16.</strong> Greedy Layer-wise Pretraining vs Transfer Learning: Understanding Deep Learning&#39;s Evolution</a></li><li class="chapter-item expanded "><a href="chapter_014.html" target="_parent"><strong aria-hidden="true">17.</strong> Freezing Transfer Learning Layers in Transformers</a></li><li class="chapter-item expanded "><a href="chapter_015.html" target="_parent"><strong aria-hidden="true">18.</strong> Dropout During Training vs Inference: The Critical Difference</a></li><li class="chapter-item expanded "><a href="chapter_024.html" target="_parent"><strong aria-hidden="true">19.</strong> The Deep Learning Renaissance: Why Neural Networks Succeeded After Decades</a></li><li class="chapter-item expanded "><a href="chapter_038.html" target="_parent"><strong aria-hidden="true">20.</strong> RNNs vs Transformers: Understanding Sequential Processing Architectures</a></li><li class="chapter-item expanded "><a href="chapter_041.html" target="_parent"><strong aria-hidden="true">21.</strong> The Most Computationally Expensive Operation in Backpropagation</a></li><li class="chapter-item expanded "><a href="chapter_045.html" target="_parent"><strong aria-hidden="true">22.</strong> Understanding the Time Complexity of Self-Attention Layers</a></li><li class="chapter-item expanded "><a href="chapter_055.html" target="_parent"><strong aria-hidden="true">23.</strong> Activation Functions: Understanding Neural Network Decision Making Without Calculations</a></li><li class="chapter-item expanded "><a href="chapter_056.html" target="_parent"><strong aria-hidden="true">24.</strong> Dead ReLU Neurons: Diagnosing and Fixing Inactive Units</a></li><li class="chapter-item expanded "><a href="chapter_064.html" target="_parent"><strong aria-hidden="true">25.</strong> Transformers Beyond Natural Language Processing: Vision Transformers and Computer Vision Applications</a></li><li class="chapter-item expanded "><a href="chapter_065.html" target="_parent"><strong aria-hidden="true">26.</strong> Adapting Pre-trained Neural Networks: From Classification to Regression</a></li><li class="chapter-item expanded "><a href="chapter_066.html" target="_parent"><strong aria-hidden="true">27.</strong> Why Neural Network Training Loss Doesn&#39;t Decrease in Early Epochs</a></li><li class="chapter-item expanded "><a href="chapter_074.html" target="_parent"><strong aria-hidden="true">28.</strong> Weighted Ensemble of Logistic Regression Models as an Artificial Neural Network</a></li><li class="chapter-item expanded "><a href="chapter_081.html" target="_parent"><strong aria-hidden="true">29.</strong> The Hidden Trap: ReLU Before Sigmoid Activation</a></li><li class="chapter-item expanded "><a href="chapter_084.html" target="_parent"><strong aria-hidden="true">30.</strong> Debugging Neural Network Training: When High Loss Meets Small Datasets</a></li><li class="chapter-item expanded "><a href="chapter_085.html" target="_parent"><strong aria-hidden="true">31.</strong> CNNs vs Fully-Connected Networks: Why Spatial Awareness Matters</a></li><li class="chapter-item expanded "><a href="chapter_086.html" target="_parent"><strong aria-hidden="true">32.</strong> Neural Network Weight Initialization: Why Identical Weights Break Everything</a></li><li class="chapter-item expanded affix "><li class="part-title">Optimization and Training</li><li class="chapter-item expanded "><a href="chapter_039.html" target="_parent"><strong aria-hidden="true">33.</strong> Does SGD Always Decrease the Loss Function?</a></li><li class="chapter-item expanded "><a href="chapter_040.html" target="_parent"><strong aria-hidden="true">34.</strong> Why Approximate Solutions in Training Are Perfectly Fine</a></li><li class="chapter-item expanded "><a href="chapter_047.html" target="_parent"><strong aria-hidden="true">35.</strong> Why Gradient Descent Instead of Analytical Solutions?</a></li><li class="chapter-item expanded "><a href="chapter_048.html" target="_parent"><strong aria-hidden="true">36.</strong> The Hessian Matrix in Optimization: Why Deep Learning Avoids Second-Order Methods</a></li><li class="chapter-item expanded "><a href="chapter_061.html" target="_parent"><strong aria-hidden="true">37.</strong> K-means Clustering: Gradient Descent vs Traditional Optimization</a></li><li class="chapter-item expanded "><a href="chapter_062.html" target="_parent"><strong aria-hidden="true">38.</strong> When is Expectation-Maximization Useful? Understanding the EM Algorithm</a></li><li class="chapter-item expanded "><a href="chapter_067.html" target="_parent"><strong aria-hidden="true">39.</strong> The Dangers of Setting Momentum Too High in SGD Optimization</a></li><li class="chapter-item expanded "><a href="chapter_070.html" target="_parent"><strong aria-hidden="true">40.</strong> Weight Decay Scaling Factors: Understanding the Relationship with Batch Size and Learning Rate</a></li><li class="chapter-item expanded "><a href="chapter_079.html" target="_parent"><strong aria-hidden="true">41.</strong> Batch Size Optimization: Understanding the Trade-offs Between Large and Small Batches</a></li><li class="chapter-item expanded "><a href="chapter_093.html" target="_parent"><strong aria-hidden="true">42.</strong> Online Learning vs Batch Learning: When Real-Time Matters</a></li><li class="chapter-item expanded "><a href="chapter_095.html" target="_parent"><strong aria-hidden="true">43.</strong> Hyperparameter Tuning: The Art and Science of Model Optimization</a></li><li class="chapter-item expanded affix "><li class="part-title">Model Evaluation and Validation</li><li class="chapter-item expanded "><a href="chapter_025.html" target="_parent"><strong aria-hidden="true">44.</strong> When Training and Testing Accuracy Converge: Understanding Model Performance</a></li><li class="chapter-item expanded "><a href="chapter_042.html" target="_parent"><strong aria-hidden="true">45.</strong> Classification with Noisy Labels: Handling Incorrect Training Data</a></li><li class="chapter-item expanded "><a href="chapter_043.html" target="_parent"><strong aria-hidden="true">46.</strong> When Perfection Becomes a Problem: Logistic Regression and Perfectly Separable Data</a></li><li class="chapter-item expanded "><a href="chapter_044.html" target="_parent"><strong aria-hidden="true">47.</strong> Debugging Production ML Models: When Great Training Performance Meets Production Reality</a></li><li class="chapter-item expanded "><a href="chapter_050.html" target="_parent"><strong aria-hidden="true">48.</strong> When A/B Tests Show No Significant Results: A Complete Guide to Next Steps</a></li><li class="chapter-item expanded "><a href="chapter_060.html" target="_parent"><strong aria-hidden="true">49.</strong> Linear Regression with Noisy Inputs: Objective Functions and Their Effects</a></li><li class="chapter-item expanded "><a href="chapter_076.html" target="_parent"><strong aria-hidden="true">50.</strong> The Bias-Variance Tradeoff: Understanding Model Complexity Through Polynomial Regression</a></li><li class="chapter-item expanded "><a href="chapter_078.html" target="_parent"><strong aria-hidden="true">51.</strong> Choosing Evaluation Metrics for Criminal Identification Systems</a></li><li class="chapter-item expanded "><a href="chapter_080.html" target="_parent"><strong aria-hidden="true">52.</strong> Theoretical Limits of Classification: When Perfect Accuracy is Impossible</a></li><li class="chapter-item expanded "><a href="chapter_082.html" target="_parent"><strong aria-hidden="true">53.</strong> Data Leakage in Class Imbalance: The Hidden Trap of Premature Duplication</a></li><li class="chapter-item expanded "><a href="chapter_090.html" target="_parent"><strong aria-hidden="true">54.</strong> Cross-Validation: The Gold Standard for Model Evaluation</a></li><li class="chapter-item expanded "><a href="chapter_094.html" target="_parent"><strong aria-hidden="true">55.</strong> Handling Class Imbalance in Classification: A Complete Guide to Balanced Machine Learning</a></li><li class="chapter-item expanded affix "><li class="part-title">Mathematical Foundations</li><li class="chapter-item expanded "><a href="chapter_019.html" target="_parent"><strong aria-hidden="true">56.</strong> Why Use Sigmoid for Numerical Prediction: Understanding Bounded Outputs</a></li><li class="chapter-item expanded "><a href="chapter_020.html" target="_parent"><strong aria-hidden="true">57.</strong> The Exponential Decay Function: A Mathematical Foundation for Machine Learning</a></li><li class="chapter-item expanded "><a href="chapter_021.html" target="_parent"><strong aria-hidden="true">58.</strong> From Binary Bits to Continuous Probabilities: Understanding the Sigmoid Function</a></li><li class="chapter-item expanded "><a href="chapter_022.html" target="_parent"><strong aria-hidden="true">59.</strong> PCA and Correlated Variables: To Remove or Not to Remove?</a></li><li class="chapter-item expanded "><a href="chapter_023.html" target="_parent"><strong aria-hidden="true">60.</strong> Understanding Dot Product Computational Complexity: How It Scales with N</a></li><li class="chapter-item expanded "><a href="chapter_010.html" target="_parent"><strong aria-hidden="true">61.</strong> Clock Angle Problems: Mathematical Reasoning in Technical Interviews</a></li><li class="chapter-item expanded "><a href="chapter_077.html" target="_parent"><strong aria-hidden="true">62.</strong> The Softmax Function and Scalar Multiplication: A Common ML Interview Misconception</a></li><li class="chapter-item expanded affix "><li class="part-title">Probability and Statistics</li><li class="chapter-item expanded "><a href="chapter_026.html" target="_parent"><strong aria-hidden="true">63.</strong> The Fair and Unfair Coin Problem: Mastering Bayesian Probability for Interviews</a></li><li class="chapter-item expanded "><a href="chapter_027.html" target="_parent"><strong aria-hidden="true">64.</strong> Expected Waiting Time for Extreme Values in Normal Distributions</a></li><li class="chapter-item expanded "><a href="chapter_028.html" target="_parent"><strong aria-hidden="true">65.</strong> Generating Fair Odds from an Unfair Coin: Von Neumann&#39;s Elegant Solution</a></li><li class="chapter-item expanded "><a href="chapter_029.html" target="_parent"><strong aria-hidden="true">66.</strong> Maximum Likelihood Estimation for Exponential Distribution: Customer Lifetime Modeling</a></li><li class="chapter-item expanded "><a href="chapter_030.html" target="_parent"><strong aria-hidden="true">67.</strong> Probability Sampling for Feature Release Decisions</a></li><li class="chapter-item expanded "><a href="chapter_031.html" target="_parent"><strong aria-hidden="true">68.</strong> The Gambler&#39;s Ruin Problem: Asymmetric Coin Flip Probability</a></li><li class="chapter-item expanded "><a href="chapter_032.html" target="_parent"><strong aria-hidden="true">69.</strong> Regression Slope Symmetry: The Hidden Mathematical Relationship</a></li><li class="chapter-item expanded affix "><li class="part-title">Machine Learning Algorithms</li><li class="chapter-item expanded "><a href="chapter_046.html" target="_parent"><strong aria-hidden="true">70.</strong> Random Forest: Understanding the Power of Randomness in Ensemble Learning</a></li><li class="chapter-item expanded "><a href="chapter_051.html" target="_parent"><strong aria-hidden="true">71.</strong> Analyzing Network Effects: When Family Members Join Social Media Platforms</a></li><li class="chapter-item expanded "><a href="chapter_052.html" target="_parent"><strong aria-hidden="true">72.</strong> Logistic Regression vs Decision Trees: When to Choose Which Algorithm</a></li><li class="chapter-item expanded "><a href="chapter_053.html" target="_parent"><strong aria-hidden="true">73.</strong> High-Dimensional Models with Poor Performance: The Curse of Dimensionality</a></li><li class="chapter-item expanded "><a href="chapter_054.html" target="_parent"><strong aria-hidden="true">74.</strong> When Should We Use Naive Bayes with Laplace Smoothing? A Complete Guide with Practical Examples</a></li><li class="chapter-item expanded "><a href="chapter_087.html" target="_parent"><strong aria-hidden="true">75.</strong> High-Dimensional Data Challenges: Navigating the Curse of Dimensionality</a></li><li class="chapter-item expanded "><a href="chapter_088.html" target="_parent"><strong aria-hidden="true">76.</strong> L1 vs L2 Regularization: Understanding Ridge, Lasso, and the Art of Model Generalization</a></li><li class="chapter-item expanded "><a href="chapter_091.html" target="_parent"><strong aria-hidden="true">77.</strong> Bagging vs Boosting: Understanding Ensemble Learning Strategies</a></li><li class="chapter-item expanded affix "><li class="part-title">Ensemble Methods</li><li class="chapter-item expanded "><a href="chapter_008.html" target="_parent"><strong aria-hidden="true">78.</strong> Why Ensembles Typically Outperform Individual Models (And When They Don&#39;t)</a></li><li class="chapter-item expanded "><a href="chapter_011.html" target="_parent"><strong aria-hidden="true">79.</strong> Optimizing Labeled Data: Three Industry-Proven Strategies</a></li><li class="chapter-item expanded "><a href="chapter_012.html" target="_parent"><strong aria-hidden="true">80.</strong> Few-Shot Learning and Meta-Learning: Learning to Learn with Limited Data</a></li><li class="chapter-item expanded affix "><li class="part-title">Generative Models and Advanced AI</li><li class="chapter-item expanded "><a href="chapter_016.html" target="_parent"><strong aria-hidden="true">81.</strong> Variational Autoencoders: Understanding the Need for Variation and Its Connection to NLU vs NLG</a></li><li class="chapter-item expanded "><a href="chapter_017.html" target="_parent"><strong aria-hidden="true">82.</strong> Generative Models: Training vs Inference in Text Generation</a></li><li class="chapter-item expanded "><a href="chapter_018.html" target="_parent"><strong aria-hidden="true">83.</strong> Subword Tokenization: Breaking Down the Building Blocks of Language</a></li><li class="chapter-item expanded "><a href="chapter_037.html" target="_parent"><strong aria-hidden="true">84.</strong> Multimodal Alignment and Cross-Modal Attention Mechanisms</a></li><li class="chapter-item expanded "><a href="chapter_058.html" target="_parent"><strong aria-hidden="true">85.</strong> Identifying Synonyms from Large Text Corpora</a></li><li class="chapter-item expanded "><a href="chapter_071.html" target="_parent"><strong aria-hidden="true">86.</strong> Fuzzy Logic: Handling Uncertainty in Intelligent Systems</a></li><li class="chapter-item expanded "><a href="chapter_072.html" target="_parent"><strong aria-hidden="true">87.</strong> Understanding Latent Variables vs Embeddings in Stable Diffusion</a></li><li class="chapter-item expanded affix "><li class="part-title">Recommendation Systems</li><li class="chapter-item expanded "><a href="chapter_006.html" target="_parent"><strong aria-hidden="true">88.</strong> Content-Based vs. Collaborative Filtering in Recommendation Systems</a></li><li class="chapter-item expanded "><a href="chapter_007.html" target="_parent"><strong aria-hidden="true">89.</strong> Building a Restaurant Recommendation System for TripAdvisor</a></li><li class="chapter-item expanded affix "><li class="part-title">Computer Vision and Object Detection</li><li class="chapter-item expanded "><a href="chapter_009.html" target="_parent"><strong aria-hidden="true">90.</strong> Focal Loss: Solving Class Imbalance in Object Detection</a></li><li class="chapter-item expanded "><a href="chapter_033.html" target="_parent"><strong aria-hidden="true">91.</strong> Face Verification Systems: Building ML-Powered Identity Solutions</a></li><li class="chapter-item expanded affix "><li class="part-title">System Design and Applications</li><li class="chapter-item expanded "><a href="chapter_034.html" target="_parent"><strong aria-hidden="true">92.</strong> Alpha-Beta Pruning: The Key Optimization for Minimax Algorithms</a></li><li class="chapter-item expanded "><a href="chapter_059.html" target="_parent"><strong aria-hidden="true">93.</strong> Modeling Airbnb New Listing Revenue: A Complete ML System Design</a></li><li class="chapter-item expanded "><a href="chapter_073.html" target="_parent"><strong aria-hidden="true">94.</strong> Building a Churn Prediction Model for Robinhood Users</a></li><li class="chapter-item expanded "><a href="chapter_083.html" target="_parent"><strong aria-hidden="true">95.</strong> Model Extrapolation and Reverse Optimization: The Car Fuel Efficiency Problem</a></li></ol>
    </body>
</html>
